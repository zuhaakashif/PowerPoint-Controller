import cv2
import mediapipe as mp
from math import hypot
import numpy as np
import os
import pyautogui
import time
import subprocess
import psutil


# Initialize the Mediapipe Hands model with parameters for detection and tracking confidence
mpHands = mp.solutions.hands
hands = mpHands.Hands(
    static_image_mode=False,  # Whether to treat the input images as a batch of static images
    model_complexity=1,       # Complexity of the hand landmark model
    min_detection_confidence=0.75,  # Minimum confidence value for hand detection
    min_tracking_confidence=0.75,   # Minimum confidence value for hand tracking
    max_num_hands=2           # Maximum number of hands to detect
)
# Initialize the drawing utility for hand landmarks
Draw = mp.solutions.drawing_utils

# Start capturing video from the webcam
cap = cv2.VideoCapture(0)

# Timestamps to control gesture debounce
last_powerpoint_time = 0
last_gesture_time = 0

# Minimum delay between gestures (in seconds)
gesture_delay = 2

def open_powerpoint():
    """Function to open a PowerPoint presentation if it's not already open."""
    # Check if PowerPoint is already running
    if not any("POWERPNT.EXE" in p.name() for p in psutil.process_iter()):
        # Change the path if PowerPoint is not in the default location
        subprocess.Popen(r"C:\\Program Files\\Microsoft Office\\root\\Office16\\POWERPNT.EXE", shell=True)
    else:
        print("PowerPoint is already running.")

def navigate_powerpoint(direction):
    """Function to navigate through PowerPoint slides."""
    if direction == "left":
        pyautogui.press('left')
    elif direction == "right":
        pyautogui.press('right')

def close_powerpoint():
    """Function to close PowerPoint."""
    pyautogui.hotkey('alt', 'f4')

def recognize_gesture(landmarkList):
    """Function to recognize specific hand gestures based on the positions of hand landmarks."""
    if len(landmarkList) >= 21:
        # Extract specific landmarks for easier access
        thumb_tip = landmarkList[4]  # Thumb tip
        index_tip = landmarkList[8]  # Index finger tip
        middle_tip = landmarkList[12]  # Middle finger tip
        ring_tip = landmarkList[16]  # Ring finger tip
        pinky_tip = landmarkList[20]  # Pinky finger tip
        thumb_ip = landmarkList[3]  # Thumb IP (interphalangeal) joint

        # Recognize "Thumbs up" gesture
        # Check if the thumb tip is above the thumb IP joint and sufficiently away from the index finger tip
        if thumb_tip[1] < thumb_ip[1] and abs(thumb_tip[1] - index_tip[1]) > 50:
            print("Thumbs detected")
            return "thumbs_up"

        # Recognize "Index Finger" pointing gesture for next slide
        # Check if the index finger tip is above the thumb tip and all other fingers are below the index finger tip
        if index_tip[2] < thumb_tip[2] and all(landmark[2] > index_tip[2] for landmark in [middle_tip, ring_tip, pinky_tip]):
            print("Index finger detected")
            return "index_finger"

        # Recognize "Ring Finger" gesture for previous slide
        # Check if the ring finger tip is above all other fingers and the thumb
        if ring_tip[2] < middle_tip[2] and ring_tip[2] < pinky_tip[2] and ring_tip[2] < index_tip[2] and ring_tip[2] < thumb_tip[2]:
            print("Ring finger detected")
            return "ring_finger"

        # Recognize "Pinky Finger" gesture to close PowerPoint
        # Check if the pinky finger tip is above all other fingers and the thumb
        if pinky_tip[2] < ring_tip[2] and pinky_tip[2] < middle_tip[2] and pinky_tip[2] < index_tip[2] and pinky_tip[2] < thumb_tip[2]:
            print("Pinky finger detected")
            return "pinky_finger"

    # Return None if no recognized gesture is detected
    return None

while True:
    # Read video frame by frame
    #ret: A boolean value that indicates whether the frame was captured successfully
    ret, frame = cap.read()

    # Check if frame is captured
    if not ret:
        print("Failed to capture image")
        continue

    # Flip the image horizontally for a later selfie-view display
    frame = cv2.flip(frame, 1)

    # Convert the BGR image to RGB image
    # OpenCV uses BGR color format by default, but Mediapipe requires RGB format for processing
    frameRGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Process the RGB image to detect and track hand landmarks
    # The process function detects the hand(s) in the frame and 
    # returns the hand landmarks (if any are detected)
    Process = hands.process(frameRGB)

    # List to store the landmark positions
    landmarkList = []
    # If hands are detected in the image (frame)
    if Process.multi_hand_landmarks:
        # Detect hand landmarks
        #handlm represents the landmarks of a single detected hand
        for handlm in Process.multi_hand_landmarks:
            # This loops through each landmark of the detected hand
            for _id, landmarks in enumerate(handlm.landmark):
                # This retrieves the dimensions of the frame
                height, width, color_channels = frame.shape
                x, y = int(landmarks.x * width), int(landmarks.y * height)
                 # This scales the normalized coordinates to the frame's dimensions
                landmarkList.append([_id, x, y])
                # appends the landmark ID and its (x, y) coordinates to the landmarkList.


            # Draw the hand landmarks on the image
            Draw.draw_landmarks(frame, handlm, mpHands.HAND_CONNECTIONS)

    # Recognize gestures and perform actions with debounce
    gesture = recognize_gesture(landmarkList)
    current_time = time.time()# preventing repeated actions from a single gesture

    # Perform actions based on recognized gestures
    if gesture == "thumbs_up" and (current_time - last_powerpoint_time) > gesture_delay:
        open_powerpoint()
        last_powerpoint_time = current_time

    elif gesture == "index_finger" and (current_time - last_gesture_time) > gesture_delay:
        navigate_powerpoint("right")
        last_gesture_time = current_time

    elif gesture == "ring_finger" and (current_time - last_gesture_time) > gesture_delay:
        navigate_powerpoint("left")
        last_gesture_time = current_time

    elif gesture == "pinky_finger" and (current_time - last_gesture_time) > gesture_delay:
        close_powerpoint()
        last_gesture_time = current_time

    # Display the video frame
    cv2.imshow('Image', frame)

    # Add a wait key to exit on 'q' or window close event
    key = cv2.waitKey(1) & 0xFF
    if key == ord('q') or cv2.getWindowProperty('Image', cv2.WND_PROP_VISIBLE) < 1:
        break

# Release the capture and destroy all windows
cap.release()
cv2.destroyAllWindows()